{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T18:53:47.636939Z",
     "iopub.status.busy": "2025-07-18T18:53:47.636609Z",
     "iopub.status.idle": "2025-07-18T18:53:55.398121Z",
     "shell.execute_reply": "2025-07-18T18:53:55.396983Z",
     "shell.execute_reply.started": "2025-07-18T18:53:47.636915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-18T18:53:55.400094Z",
     "iopub.status.busy": "2025-07-18T18:53:55.399758Z",
     "iopub.status.idle": "2025-07-18T18:53:55.799675Z",
     "shell.execute_reply": "2025-07-18T18:53:55.798676Z",
     "shell.execute_reply.started": "2025-07-18T18:53:55.400064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T18:53:55.801575Z",
     "iopub.status.busy": "2025-07-18T18:53:55.800863Z",
     "iopub.status.idle": "2025-07-18T18:54:22.053895Z",
     "shell.execute_reply": "2025-07-18T18:54:22.052703Z",
     "shell.execute_reply.started": "2025-07-18T18:53:55.801466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors, AllChem, Fragments, Lipinski\n",
    "from rdkit.Chem import rdmolops\n",
    "# Data paths\n",
    "BASE_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\n",
    "RDKIT_AVAILABLE = True\n",
    "TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "def get_canonical_smiles(smiles):\n",
    "        \"\"\"Convert SMILES to canonical form for consistency\"\"\"\n",
    "        if not RDKIT_AVAILABLE:\n",
    "            return smiles\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except:\n",
    "            pass\n",
    "        return smiles\n",
    "#Cell 3: Robust Data Loading with Complete R-Group Filtering\n",
    "\"\"\"\n",
    "Load competition data with complete filtering of problematic polymer notation\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìÇ Loading competition data...\")\n",
    "train = pd.read_csv(BASE_PATH + 'train.csv')\n",
    "test = pd.read_csv(BASE_PATH + 'test.csv')\n",
    "\n",
    "print(f\"   Training samples: {len(train)}\")\n",
    "print(f\"   Test samples: {len(test)}\")\n",
    "\n",
    "def clean_and_validate_smiles(smiles):\n",
    "    \"\"\"Completely clean and validate SMILES, removing all problematic patterns\"\"\"\n",
    "    if not isinstance(smiles, str) or len(smiles) == 0:\n",
    "        return None\n",
    "    \n",
    "    # List of all problematic patterns we've seen\n",
    "    bad_patterns = [\n",
    "        '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]', \n",
    "        \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n",
    "        # Additional patterns that cause issues\n",
    "        '([R])', '([R1])', '([R2])', \n",
    "    ]\n",
    "    \n",
    "    # Check for any bad patterns\n",
    "    for pattern in bad_patterns:\n",
    "        if pattern in smiles:\n",
    "            return None\n",
    "    \n",
    "    # Additional check: if it contains ] followed by [ without valid atoms, likely polymer notation\n",
    "    if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):\n",
    "        return None\n",
    "    \n",
    "    # Try to parse with RDKit if available\n",
    "    if RDKIT_AVAILABLE:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # If RDKit not available, return cleaned SMILES\n",
    "    return smiles\n",
    "\n",
    "# Clean and validate all SMILES\n",
    "print(\"üîÑ Cleaning and validating SMILES...\")\n",
    "train['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\n",
    "test['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n",
    "\n",
    "# Remove invalid SMILES\n",
    "invalid_train = train['SMILES'].isnull().sum()\n",
    "invalid_test = test['SMILES'].isnull().sum()\n",
    "\n",
    "print(f\"   Removed {invalid_train} invalid SMILES from training data\")\n",
    "print(f\"   Removed {invalid_test} invalid SMILES from test data\")\n",
    "\n",
    "train = train[train['SMILES'].notnull()].reset_index(drop=True)\n",
    "test = test[test['SMILES'].notnull()].reset_index(drop=True)\n",
    "\n",
    "print(f\"   Final training samples: {len(train)}\")\n",
    "print(f\"   Final test samples: {len(test)}\")\n",
    "\n",
    "def add_extra_data_clean(df_train, df_extra, target):\n",
    "    \"\"\"Add external data with thorough SMILES cleaning\"\"\"\n",
    "    n_samples_before = len(df_train[df_train[target].notnull()])\n",
    "    \n",
    "    print(f\"      Processing {len(df_extra)} {target} samples...\")\n",
    "    \n",
    "    # Clean external SMILES\n",
    "    df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)\n",
    "    \n",
    "    # Remove invalid SMILES and missing targets\n",
    "    before_filter = len(df_extra)\n",
    "    df_extra = df_extra[df_extra['SMILES'].notnull()]\n",
    "    df_extra = df_extra.dropna(subset=[target])\n",
    "    after_filter = len(df_extra)\n",
    "    \n",
    "    print(f\"      Kept {after_filter}/{before_filter} valid samples\")\n",
    "    \n",
    "    if len(df_extra) == 0:\n",
    "        print(f\"      No valid data remaining for {target}\")\n",
    "        return df_train\n",
    "    \n",
    "    # Group by canonical SMILES and average duplicates\n",
    "    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n",
    "    \n",
    "    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n",
    "    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n",
    "\n",
    "    # Fill missing values\n",
    "    filled_count = 0\n",
    "    for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():\n",
    "        if smile in cross_smiles:\n",
    "            df_train.loc[df_train['SMILES']==smile, target] = \\\n",
    "                df_extra[df_extra['SMILES']==smile][target].values[0]\n",
    "            filled_count += 1\n",
    "    \n",
    "    # Add unique SMILES\n",
    "    extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()\n",
    "    if len(extra_to_add) > 0:\n",
    "        for col in TARGETS:\n",
    "            if col not in extra_to_add.columns:\n",
    "                extra_to_add[col] = np.nan\n",
    "        \n",
    "        extra_to_add = extra_to_add[['SMILES'] + TARGETS]\n",
    "        df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)\n",
    "\n",
    "    n_samples_after = len(df_train[df_train[target].notnull()])\n",
    "    print(f'      {target}: +{n_samples_after-n_samples_before} samples, +{len(unique_smiles_extra)} unique SMILES')\n",
    "    return df_train\n",
    "\n",
    "# Load external datasets with robust error handling\n",
    "print(\"\\nüìÇ Loading external datasets...\")\n",
    "\n",
    "external_datasets = []\n",
    "\n",
    "# Function to safely load datasets\n",
    "def safe_load_dataset(path, target, processor_func, description):\n",
    "    try:\n",
    "        if path.endswith('.xlsx'):\n",
    "            data = pd.read_excel(path)\n",
    "        else:\n",
    "            data = pd.read_csv(path)\n",
    "        \n",
    "        data = processor_func(data)\n",
    "        external_datasets.append((target, data))\n",
    "        print(f\"   ‚úÖ {description}: {len(data)} samples\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è {description} failed: {str(e)[:100]}\")\n",
    "        return False\n",
    "\n",
    "# Load each dataset\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/tc-smiles/Tc_SMILES.csv',\n",
    "    'Tc',\n",
    "    lambda df: df.rename(columns={'TC_mean': 'Tc'}),\n",
    "    'Tc data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv',\n",
    "    'Tg', \n",
    "    lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,\n",
    "    'TgSS enriched data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv',\n",
    "    'Tg',\n",
    "    lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),\n",
    "    'JCIM Tg data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/smiles-extra-data/data_tg3.xlsx',\n",
    "    'Tg',\n",
    "    lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15),\n",
    "    'Xlsx Tg data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/smiles-extra-data/data_dnst1.xlsx',\n",
    "    'Density',\n",
    "    lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n",
    "                .query('SMILES.notnull() and Density.notnull() and Density != \"nylon\"')\n",
    "                .assign(Density=lambda x: x['Density'].astype(float) - 0.118),\n",
    "    'Density data'\n",
    ")\n",
    "\n",
    "safe_load_dataset(\n",
    "    '/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv',\n",
    "    'FFV', \n",
    "    lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,\n",
    "    'dataset 4'\n",
    ")\n",
    "\n",
    "# Integrate external data\n",
    "print(\"\\nüîÑ Integrating external data...\")\n",
    "train_extended = train[['SMILES'] + TARGETS].copy()\n",
    "\n",
    "for target, dataset in external_datasets:\n",
    "    print(f\"   Processing {target} data...\")\n",
    "    train_extended = add_extra_data_clean(train_extended, dataset, target)\n",
    "\n",
    "print(f\"\\nüìä Final training data:\")\n",
    "print(f\"   Original samples: {len(train)}\")\n",
    "print(f\"   Extended samples: {len(train_extended)}\")\n",
    "print(f\"   Gain: +{len(train_extended) - len(train)} samples\")\n",
    "\n",
    "for target in TARGETS:\n",
    "    count = train_extended[target].notna().sum()\n",
    "    original_count = train[target].notna().sum() if target in train.columns else 0\n",
    "    gain = count - original_count\n",
    "    print(f\"   {target}: {count:,} samples (+{gain})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data integration complete with clean SMILES!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T18:54:22.056185Z",
     "iopub.status.busy": "2025-07-18T18:54:22.055887Z",
     "iopub.status.idle": "2025-07-18T18:54:22.285371Z",
     "shell.execute_reply": "2025-07-18T18:54:22.284308Z",
     "shell.execute_reply.started": "2025-07-18T18:54:22.056162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def separate_subtables(train_df):\n",
    "\t\n",
    "\tlabels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\tsubtables = {}\n",
    "\tfor label in labels:\n",
    "\t\tsubtables[label] = train_df[['SMILES', label]][train_df[label].notna()]\n",
    "\treturn subtables\n",
    "\n",
    "def augment_smiles_dataset(smiles_list, labels, num_augments=3):\n",
    "\t\"\"\"\n",
    "\tAugments a list of SMILES strings by generating randomized versions.\n",
    "\n",
    "\tParameters:\n",
    "\t\tsmiles_list (list of str): Original SMILES strings.\n",
    "\t\tlabels (list or np.array): Corresponding labels.\n",
    "\t\tnum_augments (int): Number of augmentations per SMILES.\n",
    "\n",
    "\tReturns:\n",
    "\t\ttuple: (augmented_smiles, augmented_labels)\n",
    "\t\"\"\"\n",
    "\taugmented_smiles = []\n",
    "\taugmented_labels = []\n",
    "\n",
    "\tfor smiles, label in zip(smiles_list, labels):\n",
    "\t\tmol = Chem.MolFromSmiles(smiles)\n",
    "\t\tif mol is None:\n",
    "\t\t\tcontinue\n",
    "\t\t# Add original\n",
    "\t\taugmented_smiles.append(smiles)\n",
    "\t\taugmented_labels.append(label)\n",
    "\t\t# Add randomized versions\n",
    "\t\tfor _ in range(num_augments):\n",
    "\t\t\trand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n",
    "\t\t\taugmented_smiles.append(rand_smiles)\n",
    "\t\t\taugmented_labels.append(label)\n",
    "\n",
    "\treturn augmented_smiles, np.array(augmented_labels)\n",
    "\n",
    "from rdkit.Chem import Descriptors, MACCSkeys\n",
    "from rdkit.Chem.rdMolDescriptors import CalcTPSA, CalcNumRotatableBonds\n",
    "from rdkit.Chem.Descriptors import MolWt, MolLogP\n",
    "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "def smiles_to_combined_fingerprints_with_descriptors(smiles_list, selected_descriptors, radius=2, n_bits=128):\n",
    "    generator = GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "    atom_pair_gen = GetAtomPairGenerator(fpSize=n_bits)\n",
    "    torsion_gen = GetTopologicalTorsionGenerator(fpSize=n_bits)\n",
    "    descriptor_functions = {name: func for name, func in Descriptors.descList if name in selected_descriptors}\n",
    "    fingerprints = []\n",
    "    descriptors = []\n",
    "    valid_smiles = []\n",
    "    invalid_indices = []\n",
    "\n",
    "    for i, smiles in enumerate(smiles_list):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            # Fingerprints\n",
    "            morgan_fp = generator.GetFingerprint(mol)\n",
    "            #atom_pair_fp = atom_pair_gen.GetFingerprint(mol)\n",
    "            #torsion_fp = torsion_gen.GetFingerprint(mol)\n",
    "            maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "\n",
    "            combined_fp = np.concatenate([\n",
    "                np.array(morgan_fp),\n",
    "                #np.array(atom_pair_fp),\n",
    "                #np.array(torsion_fp),\n",
    "                np.array(maccs_fp)\n",
    "            ])\n",
    "            fingerprints.append(combined_fp)\n",
    "\n",
    "            # RDKit Descriptors\n",
    "            descriptor_values = {}\n",
    "            for name, func in descriptor_functions.items():\n",
    "                try:\n",
    "                    descriptor_values[name] = func(mol)\n",
    "                except:\n",
    "                    descriptor_values[name] = None\n",
    "\n",
    "            # Specific descriptors\n",
    "            descriptor_values['MolWt'] = MolWt(mol)\n",
    "            descriptor_values['LogP'] = MolLogP(mol)\n",
    "            descriptor_values['TPSA'] = CalcTPSA(mol)\n",
    "            descriptor_values['RotatableBonds'] = CalcNumRotatableBonds(mol)\n",
    "            descriptor_values['NumAtoms'] = mol.GetNumAtoms()\n",
    "            descriptor_values['SMILES'] = smiles\n",
    "\n",
    "            # Graph-based features\n",
    "            try:\n",
    "                adj = rdmolops.GetAdjacencyMatrix(mol)\n",
    "                G = nx.from_numpy_array(adj)\n",
    "\n",
    "                if nx.is_connected(G):\n",
    "                    descriptor_values['graph_diameter'] = nx.diameter(G)\n",
    "                    descriptor_values['avg_shortest_path'] = nx.average_shortest_path_length(G)\n",
    "                else:\n",
    "                    descriptor_values['graph_diameter'] = 0\n",
    "                    descriptor_values['avg_shortest_path'] = 0\n",
    "\n",
    "                descriptor_values['num_cycles'] = len(list(nx.cycle_basis(G)))\n",
    "            except:\n",
    "                descriptor_values['graph_diameter'] = None\n",
    "                descriptor_values['avg_shortest_path'] = None\n",
    "                descriptor_values['num_cycles'] = None\n",
    "\n",
    "            descriptors.append(descriptor_values)\n",
    "            valid_smiles.append(smiles)\n",
    "        else:\n",
    "            #fingerprints.append(np.zeros(n_bits * 3 + 167))\n",
    "            fingerprints.append(np.zeros(n_bits  + 167))\n",
    "            descriptors.append(None)\n",
    "            valid_smiles.append(None)\n",
    "            invalid_indices.append(i)\n",
    "            \n",
    "\n",
    "    return np.array(fingerprints), descriptors, valid_smiles, invalid_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T18:54:22.286818Z",
     "iopub.status.busy": "2025-07-18T18:54:22.286261Z",
     "iopub.status.idle": "2025-07-18T18:54:29.978187Z",
     "shell.execute_reply": "2025-07-18T18:54:29.976977Z",
     "shell.execute_reply.started": "2025-07-18T18:54:22.286793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from rdkit.Chem import Descriptors\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, MACCSkeys, Descriptors\n",
    "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T18:54:29.979734Z",
     "iopub.status.busy": "2025-07-18T18:54:29.979064Z",
     "iopub.status.idle": "2025-07-18T18:54:29.991774Z",
     "shell.execute_reply": "2025-07-18T18:54:29.990626Z",
     "shell.execute_reply.started": "2025-07-18T18:54:29.979708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#required_descriptors = {'MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n",
    "#required_descriptors = {'graph_diameter','num_cycles','avg_shortest_path'}\n",
    "required_descriptors = {'graph_diameter','num_cycles','avg_shortest_path','MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n",
    "#required_descriptors = {}\n",
    "\n",
    "filters = {\n",
    "    'Tg': list(set([\n",
    "        'BalabanJ','BertzCT','Chi1','Chi3n','Chi4n','EState_VSA4','EState_VSA8',\n",
    "        'FpDensityMorgan3','HallKierAlpha','Kappa3','MaxAbsEStateIndex','MolLogP',\n",
    "        'NumAmideBonds','NumHeteroatoms','NumHeterocycles','NumRotatableBonds',\n",
    "        'PEOE_VSA14','Phi','RingCount','SMR_VSA1','SPS','SlogP_VSA1','SlogP_VSA5',\n",
    "        'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState4','VSA_EState6','VSA_EState7',\n",
    "        'VSA_EState8','fr_C_O_noCOO','fr_NH1','fr_benzene','fr_bicyclic','fr_ether',\n",
    "        'fr_unbrch_alkane'\n",
    "    ]).union(required_descriptors)),\n",
    "\n",
    "    'FFV': list(set([\n",
    "        'AvgIpc','BalabanJ','BertzCT','Chi0','Chi0n','Chi0v','Chi1','Chi1n','Chi1v',\n",
    "        'Chi2n','Chi2v','Chi3n','Chi3v','Chi4n','EState_VSA10','EState_VSA5',\n",
    "        'EState_VSA7','EState_VSA8','EState_VSA9','ExactMolWt','FpDensityMorgan1',\n",
    "        'FpDensityMorgan2','FpDensityMorgan3','FractionCSP3','HallKierAlpha',\n",
    "        'HeavyAtomMolWt','Kappa1','Kappa2','Kappa3','MaxAbsEStateIndex',\n",
    "        'MaxEStateIndex','MinEStateIndex','MolLogP','MolMR','MolWt','NHOHCount',\n",
    "        'NOCount','NumAromaticHeterocycles','NumHAcceptors','NumHDonors',\n",
    "        'NumHeterocycles','NumRotatableBonds','PEOE_VSA14','RingCount','SMR_VSA1',\n",
    "        'SMR_VSA10','SMR_VSA3','SMR_VSA5','SMR_VSA6','SMR_VSA7','SMR_VSA9','SPS',\n",
    "        'SlogP_VSA1','SlogP_VSA10','SlogP_VSA11','SlogP_VSA12','SlogP_VSA2',\n",
    "        'SlogP_VSA3','SlogP_VSA4','SlogP_VSA5','SlogP_VSA6','SlogP_VSA7',\n",
    "        'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState10','VSA_EState2',\n",
    "        'VSA_EState3','VSA_EState4','VSA_EState5','VSA_EState6','VSA_EState7',\n",
    "        'VSA_EState8','VSA_EState9','fr_Ar_N','fr_C_O','fr_NH0','fr_NH1',\n",
    "        'fr_aniline','fr_ether','fr_halogen','fr_thiophene'\n",
    "    ]).union(required_descriptors)),\n",
    "\n",
    "    'Tc': list(set([\n",
    "        'BalabanJ','BertzCT','Chi0','EState_VSA5','ExactMolWt','FpDensityMorgan1',\n",
    "        'FpDensityMorgan2','FpDensityMorgan3','HeavyAtomMolWt','MinEStateIndex',\n",
    "        'MolWt','NumAtomStereoCenters','NumRotatableBonds','NumValenceElectrons',\n",
    "        'SMR_VSA10','SMR_VSA7','SPS','SlogP_VSA6','SlogP_VSA8','VSA_EState1',\n",
    "        'VSA_EState7','fr_NH1','fr_ester','fr_halogen'\n",
    "    ]).union(required_descriptors)),\n",
    "\n",
    "    'Density': list(set([\n",
    "        'BalabanJ','Chi3n','Chi3v','Chi4n','EState_VSA1','ExactMolWt',\n",
    "        'FractionCSP3','HallKierAlpha','Kappa2','MinEStateIndex','MolMR','MolWt',\n",
    "        'NumAliphaticCarbocycles','NumHAcceptors','NumHeteroatoms',\n",
    "        'NumRotatableBonds','SMR_VSA10','SMR_VSA5','SlogP_VSA12','SlogP_VSA5',\n",
    "        'TPSA','VSA_EState10','VSA_EState7','VSA_EState8'\n",
    "    ]).union(required_descriptors)),\n",
    "\n",
    "    'Rg': list(set([\n",
    "        'AvgIpc','Chi0n','Chi1v','Chi2n','Chi3v','ExactMolWt','FpDensityMorgan1',\n",
    "        'FpDensityMorgan2','FpDensityMorgan3','HallKierAlpha','HeavyAtomMolWt',\n",
    "        'Kappa3','MaxAbsEStateIndex','MolWt','NOCount','NumRotatableBonds',\n",
    "        'NumUnspecifiedAtomStereoCenters','NumValenceElectrons','PEOE_VSA14',\n",
    "        'PEOE_VSA6','SMR_VSA1','SMR_VSA5','SPS','SlogP_VSA1','SlogP_VSA2',\n",
    "        'SlogP_VSA7','SlogP_VSA8','VSA_EState1','VSA_EState8','fr_alkyl_halide',\n",
    "        'fr_halogen'\n",
    "    ]).union(required_descriptors))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA Detection and Device Configuration\n",
    "def detect_device():\n",
    "    \"\"\"\n",
    "    Detect available compute device and return appropriate configuration.\n",
    "    Falls back to CPU if CUDA is not available (e.g., on Mac).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"üöÄ CUDA detected - using GPU acceleration\")\n",
    "            return 'cuda'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        # Test if XGBoost can use CUDA\n",
    "        test_params = {'device': 'cuda', 'tree_method': 'hist'}\n",
    "        # This will fail if CUDA is not available\n",
    "        xgb.XGBRegressor(**test_params)\n",
    "        print(\"üöÄ XGBoost CUDA detected - using GPU acceleration\")\n",
    "        return 'cuda'\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CUDA not available: {str(e)[:50]}...\")\n",
    "        print(\"üñ•Ô∏è Falling back to CPU - Mac compatible mode\")\n",
    "        return 'cpu'\n",
    "\n",
    "# Detect device and set parameters accordingly\n",
    "DEVICE = detect_device()\n",
    "\n",
    "# XGBoost parameters with device detection\n",
    "XGB_PARAMS = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'mae',\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.03,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 5,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 0.01,\n",
    "    'tree_method': 'hist' if DEVICE == 'cuda' else 'hist',  # hist works on both\n",
    "    'device': DEVICE,  # Will be 'cuda' or 'cpu'\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1 if DEVICE == 'cpu' else 1,  # Use all CPU cores if on CPU\n",
    "}\n",
    "\n",
    "# CatBoost parameters with device detection\n",
    "CATBOOST_PARAMS = {\n",
    "    'iterations': 500,\n",
    "    'learning_rate': 0.07,\n",
    "    'depth': 5, \n",
    "    'l2_leaf_reg': 0.20, \n",
    "    'bagging_temperature': 0.76, \n",
    "    'random_strength': 0.010, \n",
    "    'border_count': 123,\n",
    "    'loss_function': 'MAE',\n",
    "    'eval_metric': 'MAE',\n",
    "    'verbose': False,\n",
    "    'task_type': 'GPU' if DEVICE == 'cuda' else 'CPU',  # Auto-detect\n",
    "    'random_seed': 42,\n",
    "}\n",
    "\n",
    "print(f\"üîß Device configuration: {DEVICE.upper()}\")\n",
    "print(f\"üìä XGBoost device: {XGB_PARAMS['device']}\")\n",
    "print(f\"üìä CatBoost task_type: {CATBOOST_PARAMS['task_type']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device-aware model creation functions\n",
    "def create_xgb_model(target, device='cpu'):\n",
    "    \"\"\"\n",
    "    Create XGBoost model with device-aware parameters.\n",
    "    Falls back to CPU if CUDA fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if target == \"Tg\":\n",
    "            return XGBRegressor(\n",
    "                n_estimators=2173, \n",
    "                learning_rate=0.0672418745539774, \n",
    "                max_depth=6, \n",
    "                reg_lambda=5.545520219149715, \n",
    "                random_state=4,\n",
    "                device=device,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1 if device == 'cpu' else 1\n",
    "            )\n",
    "        elif target == \"Rg\":\n",
    "            return XGBRegressor(\n",
    "                n_estimators=520, \n",
    "                learning_rate=0.07324113948440986, \n",
    "                max_depth=5, \n",
    "                reg_lambda=0.9717380315982088, \n",
    "                random_state=4,\n",
    "                device=device,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1 if device == 'cpu' else 1\n",
    "            )\n",
    "        elif target == \"FFV\":\n",
    "            return XGBRegressor(\n",
    "                n_estimators=2202, \n",
    "                learning_rate=0.07220580588586338, \n",
    "                max_depth=4, \n",
    "                reg_lambda=2.8872976032666493, \n",
    "                random_state=4,\n",
    "                device=device,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1 if device == 'cpu' else 1\n",
    "            )\n",
    "        elif target == \"Tc\":\n",
    "            return XGBRegressor(\n",
    "                n_estimators=1488, \n",
    "                learning_rate=0.010456188013762864, \n",
    "                max_depth=5, \n",
    "                reg_lambda=9.970345982204618, \n",
    "                random_state=4,\n",
    "                device=device,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1 if device == 'cpu' else 1\n",
    "            )\n",
    "        elif target == \"Density\":\n",
    "            return XGBRegressor(\n",
    "                n_estimators=1958, \n",
    "                learning_rate=0.10955287548172478, \n",
    "                max_depth=5, \n",
    "                reg_lambda=3.074470087965767, \n",
    "                random_state=4,\n",
    "                device=device,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1 if device == 'cpu' else 1\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è XGBoost {device.upper()} failed for {target}, falling back to CPU: {str(e)[:50]}...\")\n",
    "        # Fallback to CPU-only\n",
    "        return create_xgb_model(target, 'cpu')\n",
    "\n",
    "def create_catboost_model(device='cpu'):\n",
    "    \"\"\"\n",
    "    Create CatBoost model with device-aware parameters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return CatBoostRegressor(\n",
    "            iterations=500,\n",
    "            learning_rate=0.07,\n",
    "            depth=5, \n",
    "            l2_leaf_reg=0.20, \n",
    "            bagging_temperature=0.76, \n",
    "            random_strength=0.010, \n",
    "            border_count=123,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE',\n",
    "            verbose=False,\n",
    "            task_type='GPU' if device == 'cuda' else 'CPU',\n",
    "            random_seed=42,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CatBoost {device.upper()} failed, falling back to CPU: {str(e)[:50]}...\")\n",
    "        return CatBoostRegressor(\n",
    "            iterations=500,\n",
    "            learning_rate=0.07,\n",
    "            depth=5, \n",
    "            l2_leaf_reg=0.20, \n",
    "            bagging_temperature=0.76, \n",
    "            random_strength=0.010, \n",
    "            border_count=123,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE',\n",
    "            verbose=False,\n",
    "            task_type='CPU',\n",
    "            random_seed=42,\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ Device-aware model creation functions ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T18:54:29.993272Z",
     "iopub.status.busy": "2025-07-18T18:54:29.992882Z",
     "iopub.status.idle": "2025-07-18T18:54:30.212589Z",
     "shell.execute_reply": "2025-07-18T18:54:30.211302Z",
     "shell.execute_reply.started": "2025-07-18T18:54:29.993213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def augment_dataset(X, y, n_samples=1000, n_components=5, random_state=None):\n",
    "    \"\"\"\n",
    "    Augments a dataset using Gaussian Mixture Models.\n",
    "\n",
    "    Parameters:\n",
    "    - X: pd.DataFrame or np.ndarray ‚Äî feature matrix\n",
    "    - y: pd.Series or np.ndarray ‚Äî target values\n",
    "    - n_samples: int ‚Äî number of synthetic samples to generate\n",
    "    - n_components: int ‚Äî number of GMM components\n",
    "    - random_state: int ‚Äî random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - X_augmented: pd.DataFrame ‚Äî augmented feature matrix\n",
    "    - y_augmented: pd.Series ‚Äî augmented target values\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    elif not isinstance(X, pd.DataFrame):\n",
    "        raise ValueError(\"X must be a pandas DataFrame or a NumPy array\")\n",
    "\n",
    "    X.columns = X.columns.astype(str)\n",
    "\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = pd.Series(y)\n",
    "    elif not isinstance(y, pd.Series):\n",
    "        raise ValueError(\"y must be a pandas Series or a NumPy array\")\n",
    "\n",
    "    df = X.copy()\n",
    "    df['Target'] = y.values\n",
    "\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "    gmm.fit(df)\n",
    "\n",
    "    synthetic_data, _ = gmm.sample(n_samples)\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "    augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n",
    "\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T18:54:30.214111Z",
     "iopub.status.busy": "2025-07-18T18:54:30.213765Z",
     "iopub.status.idle": "2025-07-18T19:05:45.912588Z",
     "shell.execute_reply": "2025-07-18T19:05:45.911735Z",
     "shell.execute_reply.started": "2025-07-18T18:54:30.214083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df=train_extended\n",
    "test_df=test\n",
    "subtables = separate_subtables(train_df)\n",
    "\n",
    "test_smiles = test_df['SMILES'].tolist()\n",
    "test_ids = test_df['id'].values\n",
    "labels = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "#labels = ['Tc']\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "\t'id': test_ids\n",
    "})\n",
    "\n",
    "data_per_label = {}\n",
    "test_data_per_label = {}\n",
    "\n",
    "for label in labels:\n",
    "    print(f\"Processing label: {label}\")\n",
    "    print(subtables[label].head())\n",
    "    print(subtables[label].shape)\n",
    "    original_smiles = subtables[label]['SMILES'].tolist()\n",
    "    original_labels = subtables[label][label].values\n",
    "    \n",
    "    original_smiles, original_labels = augment_smiles_dataset(original_smiles, original_labels, num_augments=1)\n",
    "    fingerprints, descriptors, valid_smiles, invalid_indices = smiles_to_combined_fingerprints_with_descriptors(original_smiles, filters[label], radius=2, n_bits=128)\n",
    "    # descriptors, valid_smiles, invalid_indices\\\n",
    "    #\t =smiles_to_descriptors_with_fingerprints(original_smiles, radius=2, n_bits=128)\n",
    "    \n",
    "    X=pd.DataFrame(descriptors)\n",
    "    y = np.delete(original_labels, invalid_indices)\n",
    "    \n",
    "    # pd.DataFrame(X).to_csv(f\"./mats/{label}.csv\")\n",
    "    # pd.DataFrame(y).to_csv(f\"./mats/{label}label.csv\", header=None)\n",
    "    \n",
    "    # binned = pd.qcut(y, q=10, labels=False, duplicates='drop')\n",
    "    # pd.DataFrame(binned).to_csv(f\"./mats/{label}integerlabel.csv\", header=None, index=False)\n",
    "    X = X.filter(filters[label])\n",
    "    # Convert fingerprints array to DataFrame\n",
    "    fp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fingerprints.shape[1])])\n",
    "    \n",
    "    print(fp_df.shape)\n",
    "    # Reset index to align with X\n",
    "    fp_df.reset_index(drop=True, inplace=True)\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    # Concatenate descriptors and fingerprints\n",
    "    X = pd.concat([X, fp_df], axis=1)\n",
    "    \n",
    "    print(f\"After concat: {X.shape}\")\n",
    "    \n",
    "    # Set the variance threshold\n",
    "    threshold = 0.01\n",
    "    \n",
    "    # Apply VarianceThreshold\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    \n",
    "    X = selector.fit_transform(X)\n",
    "    \n",
    "    print(f\"After variance cut: {X.shape}\")\n",
    "    \n",
    "    \n",
    "    n_samples = 1000\n",
    "    \n",
    "    data_per_label[label] = augment_dataset(X, y, n_samples=n_samples)\n",
    "    print(f\"After augment cut: {data_per_label[label].shape}\")\n",
    "    \n",
    "    fingerprints, descriptors, valid_smiles, invalid_indices = smiles_to_combined_fingerprints_with_descriptors(test_smiles, filters[label], radius=2, n_bits=128)\n",
    "    \n",
    "    test = pd.DataFrame(descriptors)\n",
    "    \n",
    "    test = test.filter(filters[label])\n",
    "    \n",
    "    # Convert fingerprints array to DataFrame\n",
    "    fp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fingerprints.shape[1])])\n",
    "    \n",
    "    # Reset index to align with X\n",
    "    fp_df.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Concatenate descriptors and fingerprints\n",
    "    test = pd.concat([test, fp_df], axis=1)\n",
    "    test = selector.transform(test)\n",
    "    \n",
    "    print(test.shape)\n",
    "    test_data_per_label[label] = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T19:05:45.913732Z",
     "iopub.status.busy": "2025-07-18T19:05:45.913501Z",
     "iopub.status.idle": "2025-07-18T19:05:45.919687Z",
     "shell.execute_reply": "2025-07-18T19:05:45.918664Z",
     "shell.execute_reply.started": "2025-07-18T19:05:45.913713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data_per_label['Tg'] = pd.DataFrame(test_data_per_label['Tg'])\n",
    "test_data_per_label['FFV'] = pd.DataFrame(test_data_per_label['FFV'])\n",
    "test_data_per_label['Tc'] = pd.DataFrame(test_data_per_label['Tc'])\n",
    "test_data_per_label['Density'] = pd.DataFrame(test_data_per_label['Density'])\n",
    "test_data_per_label['Rg'] = pd.DataFrame(test_data_per_label['Rg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T19:05:45.922386Z",
     "iopub.status.busy": "2025-07-18T19:05:45.922099Z",
     "iopub.status.idle": "2025-07-18T19:05:45.948544Z",
     "shell.execute_reply": "2025-07-18T19:05:45.947648Z",
     "shell.execute_reply.started": "2025-07-18T19:05:45.922364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_per_label['Tg'].rename(columns={'Target': 'Tg'}, inplace=True)\n",
    "data_per_label['FFV'].rename(columns={'Target': 'FFV'}, inplace=True)\n",
    "data_per_label['Tc'].rename(columns={'Target': 'Tc'}, inplace=True)\n",
    "data_per_label['Density'].rename(columns={'Target': 'Density'}, inplace=True)\n",
    "data_per_label['Rg'].rename(columns={'Target': 'Rg'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T19:05:45.949834Z",
     "iopub.status.busy": "2025-07-18T19:05:45.949581Z",
     "iopub.status.idle": "2025-07-18T19:05:45.998971Z",
     "shell.execute_reply": "2025-07-18T19:05:45.99771Z",
     "shell.execute_reply.started": "2025-07-18T19:05:45.949814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_per_label['Rg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T19:05:46.000305Z",
     "iopub.status.busy": "2025-07-18T19:05:45.999969Z",
     "iopub.status.idle": "2025-07-18T19:05:46.006947Z",
     "shell.execute_reply": "2025-07-18T19:05:46.005968Z",
     "shell.execute_reply.started": "2025-07-18T19:05:46.000281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_per_label['Density'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T19:05:46.008811Z",
     "iopub.status.busy": "2025-07-18T19:05:46.008379Z",
     "iopub.status.idle": "2025-07-18T19:05:46.029386Z",
     "shell.execute_reply": "2025-07-18T19:05:46.02841Z",
     "shell.execute_reply.started": "2025-07-18T19:05:46.008787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "XGB_PARAMS = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'mae',\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.03,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 5,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 0.01,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',  # or 'cpu' if no GPU\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "\n",
    "HGBR_PARAMS = {\n",
    "    'loss': 'absolute_error',   # same as MAE\n",
    "    'max_depth': 12,\n",
    "    'learning_rate': 0.03,\n",
    "    'max_iter': 600,\n",
    "    'early_stopping': True,\n",
    "    'validation_fraction': 0.1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "LGBM_PARAMS = {\n",
    "    'device_type': 'cpu',\n",
    "    'n_estimators': 1_000,\n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'mae',\n",
    "    'verbosity': -1,\n",
    "    'num_leaves': 50,\n",
    "    'min_data_in_leaf': 2,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_bin': 500,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 2,\n",
    "    'lambda_l2': 2,\n",
    "}\n",
    "\n",
    "EXTRATREES_PARAMS = {\n",
    "       'n_estimators': 100, \n",
    "        'max_depth': 8, \n",
    "        'min_samples_split': 3, \n",
    "        'min_samples_leaf': 1, \n",
    "        'max_features': 'auto', \n",
    "        'bootstrap': False,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "}\n",
    "CATBOOST_PARAMS = {\n",
    "        'iterations': 500,  # reduce for speed\n",
    "        'learning_rate': 0.07,\n",
    "        'depth': 5, \n",
    "        'l2_leaf_reg': 0.20, \n",
    "        'bagging_temperature': 0.76, \n",
    "        'random_strength': 0.010, \n",
    "        'border_count': 123,\n",
    "        'loss_function': 'MAE',\n",
    "        'eval_metric': 'MAE',\n",
    "        'verbose': False,\n",
    "        'task_type': 'CPU',  # ‚úÖ use CPU\n",
    "        'random_seed': 42,\n",
    "}\n",
    "LASSO_PARAMS = {\n",
    "    'alpha': 0.001,      # regularization strength\n",
    "    'max_iter': 1000,\n",
    "    'random_state': 42\n",
    "}\n",
    "SVR_PARAMS = {\n",
    "    'C': 1.0,\n",
    "    'epsilon': 0.1,\n",
    "    'kernel': 'rbf'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T19:05:46.030469Z",
     "iopub.status.busy": "2025-07-18T19:05:46.03013Z",
     "iopub.status.idle": "2025-07-18T19:13:06.613346Z",
     "shell.execute_reply": "2025-07-18T19:13:06.612179Z",
     "shell.execute_reply.started": "2025-07-18T19:05:46.03044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TARGET_VARIABLES = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "RANDOM_STATE = 42\n",
    "gbdt_oof_df = {}\n",
    "gbdt_predictions_df = pd.DataFrame({'id': test_df['id']})\n",
    "mae_scores = {}\n",
    "\n",
    "for target in TARGET_VARIABLES:\n",
    "    X_test = test_data_per_label[target]\n",
    "    X = data_per_label[target]\n",
    "    print(f\"  Training for {target}...\")\n",
    "    \n",
    "    y = data_per_label[target][target].dropna()\n",
    "    X_subset = X.loc[y.index]\n",
    "    X_test = X_test.to_numpy()\n",
    "    cols = X_subset.drop(columns=[target]).columns\n",
    "    print(f\"Size for data is {X_subset.shape} and features are {len(cols)}\")\n",
    "    \n",
    "    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(\n",
    "        X_subset.drop(columns=[target]), y, test_size=0.2, random_state=10\n",
    "    )\n",
    "    \n",
    "    X_train_fold = X_train_fold.to_numpy()\n",
    "    X_val_fold = X_val_fold.to_numpy()\n",
    "    y_train_fold = y_train_fold.to_numpy()\n",
    "    y_val_fold = y_val_fold.to_numpy()\n",
    "\n",
    "    # XGBoost\n",
    "    if target == \"Tg\":\n",
    "        xgb_model = XGBRegressor(n_estimators= 2173, learning_rate= 0.0672418745539774, max_depth= 6, reg_lambda= 5.545520219149715, random_state = 4)\n",
    "    elif target == \"Rg\":\n",
    "        xgb_model = XGBRegressor(n_estimators= 520, learning_rate= 0.07324113948440986, max_depth= 5, reg_lambda=0.9717380315982088, random_state = 4)\n",
    "    elif target == \"FFV\":\n",
    "        xgb_model = XGBRegressor(n_estimators= 2202, learning_rate= 0.07220580588586338, max_depth= 4, reg_lambda= 2.8872976032666493, random_state = 4)\n",
    "    elif target == \"Tc\":\n",
    "        xgb_model = XGBRegressor(n_estimators= 1488, learning_rate= 0.010456188013762864, max_depth= 5, reg_lambda= 9.970345982204618, random_state = 4)\n",
    "    elif target == \"Density\":\n",
    "        xgb_model = XGBRegressor(n_estimators= 1958, learning_rate= 0.10955287548172478, max_depth= 5, reg_lambda= 3.074470087965767, random_state = 4)\n",
    "\n",
    "    xgb_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], verbose=False)\n",
    "    xgb_oof_preds = xgb_model.predict(X_val_fold)\n",
    "    test_preds_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "    # # LightGBM\n",
    "    # lgb_model = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "    # lgb_model.fit(X_train_fold, y_train_fold,\n",
    "    #               eval_set=[(X_val_fold, y_val_fold)],\n",
    "    #               callbacks=[lgb.early_stopping(stopping_rounds=30, verbose=False)])\n",
    "    # oof_preds_lgb = lgb_model.predict(X_val_fold)\n",
    "    # test_preds_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "    # # Extra Trees\n",
    "    # et_model = ExtraTreesRegressor()\n",
    "    # et_model.fit(X_train_fold, y_train_fold)\n",
    "    # oof_preds_et = et_model.predict(X_val_fold)\n",
    "    # test_preds_et = et_model.predict(X_test)\n",
    "\n",
    "    # # CatBoost\n",
    "    # cat_model = CatBoostRegressor(verbose=0)\n",
    "    # cat_model.fit(X_train_fold, y_train_fold, eval_set=(X_val_fold, y_val_fold))\n",
    "    # oof_preds_cat = cat_model.predict(X_val_fold)\n",
    "    # test_preds_cat = cat_model.predict(X_test)\n",
    "\n",
    "\n",
    "    # # Lasso\n",
    "    # lasso_model = Lasso()\n",
    "    # lasso_model.fit(X_train_fold, y_train_fold)\n",
    "    # oof_preds_lasso = lasso_model.predict(X_val_fold)\n",
    "    # test_preds_lasso = lasso_model.predict(X_test)\n",
    "\n",
    "    # # Random Forest\n",
    "    rff_model = RandomForestRegressor(random_state = 42)\n",
    "    rff_model.fit(X_train_fold, y_train_fold)\n",
    "    oof_preds_rff = rff_model.predict(X_val_fold)\n",
    "    test_preds_rff = rff_model.predict(X_test)\n",
    "\n",
    "    # rff_model = RandomForestRegressor(random_state = 42)\n",
    "    # rff_model.fit(X_train_fold, y_train_fold)\n",
    "    # oof_preds_rff1 = rff_model.predict(X_val_fold)\n",
    "    # test_preds_rff1 = rff_model.predict(X_test)\n",
    "\n",
    "    # rff_model = RandomForestRegressor(random_state = 21)\n",
    "    # rff_model.fit(X_train_fold, y_train_fold)\n",
    "    # oof_preds_rff2 = rff_model.predict(X_val_fold)\n",
    "    # test_preds_rff2 = rff_model.predict(X_test)\n",
    "    \n",
    "    # Averaging predictions\n",
    "    val_preds_all = np.vstack([\n",
    "        xgb_oof_preds,\n",
    "        # oof_preds_lgb,\n",
    "         #oof_preds_et,\n",
    "        #oof_preds_cat,\n",
    "        #oof_preds_lasso,\n",
    "        oof_preds_rff,\n",
    "        # oof_preds_rff1,\n",
    "        # oof_preds_rff2\n",
    "    ])\n",
    "    final_oof_preds = np.mean(val_preds_all, axis=0)\n",
    "\n",
    "    test_preds_all = np.vstack([\n",
    "        test_preds_xgb,\n",
    "        # test_preds_lgb,\n",
    "        #test_preds_et,\n",
    "        #test_preds_cat,\n",
    "        #test_preds_lasso,\n",
    "        test_preds_rff,\n",
    "        # test_preds_rff1,\n",
    "        # test_preds_rff2\n",
    "    ])\n",
    "    final_test_preds = np.mean(test_preds_all, axis=0)\n",
    "\n",
    "    # Save predictions\n",
    "    gbdt_predictions_df[target] = final_test_preds\n",
    "    gbdt_oof_df[target] = final_oof_preds\n",
    "\n",
    "    # MAE Computation\n",
    "    mae = mean_absolute_error(y_val_fold, final_oof_preds)\n",
    "    mae_scores[target] = mae\n",
    "    print(f\"    MAE on validation set for {target}: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T19:17:51.171443Z",
     "iopub.status.busy": "2025-07-18T19:17:51.171062Z",
     "iopub.status.idle": "2025-07-18T19:17:51.185486Z",
     "shell.execute_reply": "2025-07-18T19:17:51.184533Z",
     "shell.execute_reply.started": "2025-07-18T19:17:51.171415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gbdt_predictions_df.to_csv('submission.csv', index=False)\n",
    "gbdt_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12966160,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 7678100,
     "sourceId": 12189904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7690162,
     "sourceId": 12207625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7709500,
     "sourceId": 12235747,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7709869,
     "sourceId": 12330396,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
