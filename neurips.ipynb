{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":12189904,"sourceType":"datasetVersion","datasetId":7678100},{"sourceId":12207625,"sourceType":"datasetVersion","datasetId":7690162},{"sourceId":12235747,"sourceType":"datasetVersion","datasetId":7709500},{"sourceId":12330396,"sourceType":"datasetVersion","datasetId":7709869}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:38:36.568787Z","iopub.execute_input":"2025-07-16T12:38:36.569216Z","iopub.status.idle":"2025-07-16T12:38:36.588551Z","shell.execute_reply.started":"2025-07-16T12:38:36.569177Z","shell.execute_reply":"2025-07-16T12:38:36.587303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:38:36.590117Z","iopub.execute_input":"2025-07-16T12:38:36.590693Z","iopub.status.idle":"2025-07-16T12:38:41.117238Z","shell.execute_reply.started":"2025-07-16T12:38:36.590654Z","shell.execute_reply":"2025-07-16T12:38:41.115756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rdkit import Chem\nfrom rdkit.Chem import Descriptors, rdMolDescriptors, AllChem, Fragments, Lipinski\nfrom rdkit.Chem import rdmolops\n# Data paths\nBASE_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\nRDKIT_AVAILABLE = True\nTARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\ndef get_canonical_smiles(smiles):\n        \"\"\"Convert SMILES to canonical form for consistency\"\"\"\n        if not RDKIT_AVAILABLE:\n            return smiles\n        try:\n            mol = Chem.MolFromSmiles(smiles)\n            if mol:\n                return Chem.MolToSmiles(mol, canonical=True)\n        except:\n            pass\n        return smiles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:38:41.118824Z","iopub.execute_input":"2025-07-16T12:38:41.119184Z","iopub.status.idle":"2025-07-16T12:38:41.129089Z","shell.execute_reply.started":"2025-07-16T12:38:41.11915Z","shell.execute_reply":"2025-07-16T12:38:41.127776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Cell 3: Robust Data Loading with Complete R-Group Filtering\n\"\"\"\nLoad competition data with complete filtering of problematic polymer notation\n\"\"\"\n\nprint(\"üìÇ Loading competition data...\")\ntrain = pd.read_csv(BASE_PATH + 'train.csv')\ntest = pd.read_csv(BASE_PATH + 'test.csv')\n\nprint(f\"   Training samples: {len(train)}\")\nprint(f\"   Test samples: {len(test)}\")\n\ndef clean_and_validate_smiles(smiles):\n    \"\"\"Completely clean and validate SMILES, removing all problematic patterns\"\"\"\n    if not isinstance(smiles, str) or len(smiles) == 0:\n        return None\n    \n    # List of all problematic patterns we've seen\n    bad_patterns = [\n        '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]', \n        \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n        # Additional patterns that cause issues\n        '([R])', '([R1])', '([R2])', \n    ]\n    \n    # Check for any bad patterns\n    for pattern in bad_patterns:\n        if pattern in smiles:\n            return None\n    \n    # Additional check: if it contains ] followed by [ without valid atoms, likely polymer notation\n    if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):\n        return None\n    \n    # Try to parse with RDKit if available\n    if RDKIT_AVAILABLE:\n        try:\n            mol = Chem.MolFromSmiles(smiles)\n            if mol is not None:\n                return Chem.MolToSmiles(mol, canonical=True)\n            else:\n                return None\n        except:\n            return None\n    \n    # If RDKit not available, return cleaned SMILES\n    return smiles\n\n# Clean and validate all SMILES\nprint(\"üîÑ Cleaning and validating SMILES...\")\ntrain['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\ntest['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n\n# Remove invalid SMILES\ninvalid_train = train['SMILES'].isnull().sum()\ninvalid_test = test['SMILES'].isnull().sum()\n\nprint(f\"   Removed {invalid_train} invalid SMILES from training data\")\nprint(f\"   Removed {invalid_test} invalid SMILES from test data\")\n\ntrain = train[train['SMILES'].notnull()].reset_index(drop=True)\ntest = test[test['SMILES'].notnull()].reset_index(drop=True)\n\nprint(f\"   Final training samples: {len(train)}\")\nprint(f\"   Final test samples: {len(test)}\")\n\ndef add_extra_data_clean(df_train, df_extra, target):\n    \"\"\"Add external data with thorough SMILES cleaning\"\"\"\n    n_samples_before = len(df_train[df_train[target].notnull()])\n    \n    print(f\"      Processing {len(df_extra)} {target} samples...\")\n    \n    # Clean external SMILES\n    df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)\n    \n    # Remove invalid SMILES and missing targets\n    before_filter = len(df_extra)\n    df_extra = df_extra[df_extra['SMILES'].notnull()]\n    df_extra = df_extra.dropna(subset=[target])\n    after_filter = len(df_extra)\n    \n    print(f\"      Kept {after_filter}/{before_filter} valid samples\")\n    \n    if len(df_extra) == 0:\n        print(f\"      No valid data remaining for {target}\")\n        return df_train\n    \n    # Group by canonical SMILES and average duplicates\n    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n    \n    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n\n    # Fill missing values\n    filled_count = 0\n    for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():\n        if smile in cross_smiles:\n            df_train.loc[df_train['SMILES']==smile, target] = \\\n                df_extra[df_extra['SMILES']==smile][target].values[0]\n            filled_count += 1\n    \n    # Add unique SMILES\n    extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()\n    if len(extra_to_add) > 0:\n        for col in TARGETS:\n            if col not in extra_to_add.columns:\n                extra_to_add[col] = np.nan\n        \n        extra_to_add = extra_to_add[['SMILES'] + TARGETS]\n        df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)\n\n    n_samples_after = len(df_train[df_train[target].notnull()])\n    print(f'      {target}: +{n_samples_after-n_samples_before} samples, +{len(unique_smiles_extra)} unique SMILES')\n    return df_train\n\n# Load external datasets with robust error handling\nprint(\"\\nüìÇ Loading external datasets...\")\n\nexternal_datasets = []\n\n# Function to safely load datasets\ndef safe_load_dataset(path, target, processor_func, description):\n    try:\n        if path.endswith('.xlsx'):\n            data = pd.read_excel(path)\n        else:\n            data = pd.read_csv(path)\n        \n        data = processor_func(data)\n        external_datasets.append((target, data))\n        print(f\"   ‚úÖ {description}: {len(data)} samples\")\n        return True\n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è {description} failed: {str(e)[:100]}\")\n        return False\n\n# Load each dataset\nsafe_load_dataset(\n    '/kaggle/input/tc-smiles/Tc_SMILES.csv',\n    'Tc',\n    lambda df: df.rename(columns={'TC_mean': 'Tc'}),\n    'Tc data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv',\n    'Tg', \n    lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,\n    'TgSS enriched data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv',\n    'Tg',\n    lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),\n    'JCIM Tg data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/data_tg3.xlsx',\n    'Tg',\n    lambda df: df.rename(columns={'Tg [K]': 'Tg'}).assign(Tg=lambda x: x['Tg'] - 273.15),\n    'Xlsx Tg data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/smiles-extra-data/data_dnst1.xlsx',\n    'Density',\n    lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]\n                .query('SMILES.notnull() and Density.notnull() and Density != \"nylon\"')\n                .assign(Density=lambda x: x['Density'].astype(float) - 0.118),\n    'Density data'\n)\n\nsafe_load_dataset(\n    '/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv',\n    'FFV', \n    lambda df: df[['SMILES', 'FFV']] if 'FFV' in df.columns else df,\n    'dataset 4'\n)\n\n# Integrate external data\nprint(\"\\nüîÑ Integrating external data...\")\ntrain_extended = train[['SMILES'] + TARGETS].copy()\n\nfor target, dataset in external_datasets:\n    print(f\"   Processing {target} data...\")\n    train_extended = add_extra_data_clean(train_extended, dataset, target)\n\nprint(f\"\\nüìä Final training data:\")\nprint(f\"   Original samples: {len(train)}\")\nprint(f\"   Extended samples: {len(train_extended)}\")\nprint(f\"   Gain: +{len(train_extended) - len(train)} samples\")\n\nfor target in TARGETS:\n    count = train_extended[target].notna().sum()\n    original_count = train[target].notna().sum() if target in train.columns else 0\n    gain = count - original_count\n    print(f\"   {target}: {count:,} samples (+{gain})\")\n\nprint(f\"\\n‚úÖ Data integration complete with clean SMILES!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:38:41.132174Z","iopub.execute_input":"2025-07-16T12:38:41.132525Z","iopub.status.idle":"2025-07-16T12:39:07.986664Z","shell.execute_reply.started":"2025-07-16T12:38:41.132493Z","shell.execute_reply":"2025-07-16T12:39:07.985537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef separate_subtables(train_df):\n\t\n\tlabels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\tsubtables = {}\n\tfor label in labels:\n\t\tsubtables[label] = train_df[['SMILES', label]][train_df[label].notna()]\n\treturn subtables\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:39:07.98753Z","iopub.execute_input":"2025-07-16T12:39:07.987814Z","iopub.status.idle":"2025-07-16T12:39:07.993483Z","shell.execute_reply.started":"2025-07-16T12:39:07.987793Z","shell.execute_reply":"2025-07-16T12:39:07.992315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef augment_smiles_dataset(smiles_list, labels, num_augments=3):\n\t\"\"\"\n\tAugments a list of SMILES strings by generating randomized versions.\n\n\tParameters:\n\t\tsmiles_list (list of str): Original SMILES strings.\n\t\tlabels (list or np.array): Corresponding labels.\n\t\tnum_augments (int): Number of augmentations per SMILES.\n\n\tReturns:\n\t\ttuple: (augmented_smiles, augmented_labels)\n\t\"\"\"\n\taugmented_smiles = []\n\taugmented_labels = []\n\n\tfor smiles, label in zip(smiles_list, labels):\n\t\tmol = Chem.MolFromSmiles(smiles)\n\t\tif mol is None:\n\t\t\tcontinue\n\t\t# Add original\n\t\taugmented_smiles.append(smiles)\n\t\taugmented_labels.append(label)\n\t\t# Add randomized versions\n\t\tfor _ in range(num_augments):\n\t\t\trand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n\t\t\taugmented_smiles.append(rand_smiles)\n\t\t\taugmented_labels.append(label)\n\n\treturn augmented_smiles, np.array(augmented_labels)\n\nfrom rdkit.Chem import Descriptors, MACCSkeys\nfrom rdkit.Chem.rdMolDescriptors import CalcTPSA, CalcNumRotatableBonds\nfrom rdkit.Chem.Descriptors import MolWt, MolLogP\nfrom rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\n\nimport networkx as nx\ndef smiles_to_combined_fingerprints_with_descriptors(smiles_list, radius=2, n_bits=128):\n    generator = GetMorganGenerator(radius=radius, fpSize=n_bits)\n    atom_pair_gen = GetAtomPairGenerator(fpSize=n_bits)\n    torsion_gen = GetTopologicalTorsionGenerator(fpSize=n_bits)\n\n    fingerprints = []\n    descriptors = []\n    valid_smiles = []\n    invalid_indices = []\n\n    for i, smiles in enumerate(smiles_list):\n        mol = Chem.MolFromSmiles(smiles)\n        if mol:\n            # Fingerprints\n            morgan_fp = generator.GetFingerprint(mol)\n            #atom_pair_fp = atom_pair_gen.GetFingerprint(mol)\n            #torsion_fp = torsion_gen.GetFingerprint(mol)\n            maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n\n            combined_fp = np.concatenate([\n                np.array(morgan_fp),\n                #np.array(atom_pair_fp),\n                #np.array(torsion_fp),\n                np.array(maccs_fp)\n            ])\n            fingerprints.append(combined_fp)\n\n            # RDKit Descriptors\n            descriptor_values = {}\n            for name, func in Descriptors.descList:\n                try:\n                    descriptor_values[name] = func(mol)\n                except:\n                    descriptor_values[name] = None\n\n            # Specific descriptors\n            descriptor_values['MolWt'] = MolWt(mol)\n            descriptor_values['LogP'] = MolLogP(mol)\n            descriptor_values['TPSA'] = CalcTPSA(mol)\n            descriptor_values['RotatableBonds'] = CalcNumRotatableBonds(mol)\n            descriptor_values['NumAtoms'] = mol.GetNumAtoms()\n            descriptor_values['SMILES'] = smiles\n\n            # Graph-based features\n            try:\n                adj = rdmolops.GetAdjacencyMatrix(mol)\n                G = nx.from_numpy_array(adj)\n\n                if nx.is_connected(G):\n                    descriptor_values['graph_diameter'] = nx.diameter(G)\n                    descriptor_values['avg_shortest_path'] = nx.average_shortest_path_length(G)\n                else:\n                    descriptor_values['graph_diameter'] = 0\n                    descriptor_values['avg_shortest_path'] = 0\n\n                descriptor_values['num_cycles'] = len(list(nx.cycle_basis(G)))\n            except:\n                descriptor_values['graph_diameter'] = None\n                descriptor_values['avg_shortest_path'] = None\n                descriptor_values['num_cycles'] = None\n\n            descriptors.append(descriptor_values)\n            valid_smiles.append(smiles)\n        else:\n            #fingerprints.append(np.zeros(n_bits * 3 + 167))\n            fingerprints.append(np.zeros(n_bits  + 167))\n            descriptors.append(None)\n            valid_smiles.append(None)\n            invalid_indices.append(i)\n\n    return np.array(fingerprints), descriptors, valid_smiles, invalid_indices\n\ndef smiles_to_combined_fingerprints_with_descriptorsOriginal(smiles_list, radius=2, n_bits=128):\n    generator = GetMorganGenerator(radius=radius, fpSize=n_bits)\n    atom_pair_gen = GetAtomPairGenerator(fpSize=n_bits)\n    torsion_gen = GetTopologicalTorsionGenerator(fpSize=n_bits)\n\n    fingerprints = []\n    descriptors = []\n    valid_smiles = []\n    invalid_indices = []\n\n    for i, smiles in enumerate(smiles_list):\n        mol = Chem.MolFromSmiles(smiles)\n        if mol:\n            # Fingerprints\n            morgan_fp = generator.GetFingerprint(mol)\n            #atom_pair_fp = atom_pair_gen.GetFingerprint(mol)\n            #torsion_fp = torsion_gen.GetFingerprint(mol)\n            maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n\n            combined_fp = np.concatenate([\n                np.array(morgan_fp),\n                #np.array(atom_pair_fp),\n                #np.array(torsion_fp),\n                np.array(maccs_fp)\n            ])\n            fingerprints.append(combined_fp)\n\n            # All RDKit Descriptors\n            descriptor_values = {}\n            for name, func in Descriptors.descList:\n                try:\n                    descriptor_values[name] = func(mol)\n                except:\n                    descriptor_values[name] = None\n\n            # Add specific descriptors explicitly\n            descriptor_values['MolWt'] = MolWt(mol)\n            descriptor_values['LogP'] = MolLogP(mol)\n            descriptor_values['TPSA'] = CalcTPSA(mol)\n            descriptor_values['RotatableBonds'] = CalcNumRotatableBonds(mol)\n            descriptor_values['NumAtoms'] = mol.GetNumAtoms()\n            descriptor_values['SMILES'] = smiles\n            #descriptor_values['RadiusOfGyration'] =CalcRadiusOfGyration(mol)\n\n            descriptors.append(descriptor_values)\n            valid_smiles.append(smiles)\n        else:\n            #fingerprints.append(np.zeros(n_bits * 3 + 167))\n            fingerprints.append(np.zeros( 167))\n            descriptors.append(None)\n            valid_smiles.append(None)\n            invalid_indices.append(i)\n\n    return np.array(fingerprints), descriptors, valid_smiles, invalid_indices\n\ndef make_smile_canonical(smile): # To avoid duplicates, for example: canonical '*C=C(*)C' == '*C(=C*)C'\n\ttry:\n\t\tmol = Chem.MolFromSmiles(smile)\n\t\tcanon_smile = Chem.MolToSmiles(mol, canonical=True)\n\t\treturn canon_smile\n\texcept:\n\t\treturn np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:39:07.994993Z","iopub.execute_input":"2025-07-16T12:39:07.995358Z","iopub.status.idle":"2025-07-16T12:39:08.019852Z","shell.execute_reply.started":"2025-07-16T12:39:07.995325Z","shell.execute_reply":"2025-07-16T12:39:08.018987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom rdkit import Chem\nfrom rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\nfrom rdkit.Chem import MACCSkeys\nfrom rdkit.Chem import Descriptors\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem, MACCSkeys, Descriptors\nfrom rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator, GetAtomPairGenerator, GetTopologicalTorsionGenerator\nimport numpy as np\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_absolute_error","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:39:08.020845Z","iopub.execute_input":"2025-07-16T12:39:08.021121Z","iopub.status.idle":"2025-07-16T12:39:08.048735Z","shell.execute_reply.started":"2025-07-16T12:39:08.0211Z","shell.execute_reply":"2025-07-16T12:39:08.047514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#required_descriptors = {'MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n#required_descriptors = {'graph_diameter','num_cycles','avg_shortest_path'}\nrequired_descriptors = {'graph_diameter','num_cycles','avg_shortest_path','MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n#required_descriptors = {}\n\nfilters = {\n    'Tg': list(set([\n        'BalabanJ','BertzCT','Chi1','Chi3n','Chi4n','EState_VSA4','EState_VSA8',\n        'FpDensityMorgan3','HallKierAlpha','Kappa3','MaxAbsEStateIndex','MolLogP',\n        'NumAmideBonds','NumHeteroatoms','NumHeterocycles','NumRotatableBonds',\n        'PEOE_VSA14','Phi','RingCount','SMR_VSA1','SPS','SlogP_VSA1','SlogP_VSA5',\n        'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState4','VSA_EState6','VSA_EState7',\n        'VSA_EState8','fr_C_O_noCOO','fr_NH1','fr_benzene','fr_bicyclic','fr_ether',\n        'fr_unbrch_alkane'\n    ]).union(required_descriptors)),\n\n    'FFV': list(set([\n        'AvgIpc','BalabanJ','BertzCT','Chi0','Chi0n','Chi0v','Chi1','Chi1n','Chi1v',\n        'Chi2n','Chi2v','Chi3n','Chi3v','Chi4n','EState_VSA10','EState_VSA5',\n        'EState_VSA7','EState_VSA8','EState_VSA9','ExactMolWt','FpDensityMorgan1',\n        'FpDensityMorgan2','FpDensityMorgan3','FractionCSP3','HallKierAlpha',\n        'HeavyAtomMolWt','Kappa1','Kappa2','Kappa3','MaxAbsEStateIndex',\n        'MaxEStateIndex','MinEStateIndex','MolLogP','MolMR','MolWt','NHOHCount',\n        'NOCount','NumAromaticHeterocycles','NumHAcceptors','NumHDonors',\n        'NumHeterocycles','NumRotatableBonds','PEOE_VSA14','RingCount','SMR_VSA1',\n        'SMR_VSA10','SMR_VSA3','SMR_VSA5','SMR_VSA6','SMR_VSA7','SMR_VSA9','SPS',\n        'SlogP_VSA1','SlogP_VSA10','SlogP_VSA11','SlogP_VSA12','SlogP_VSA2',\n        'SlogP_VSA3','SlogP_VSA4','SlogP_VSA5','SlogP_VSA6','SlogP_VSA7',\n        'SlogP_VSA8','TPSA','VSA_EState1','VSA_EState10','VSA_EState2',\n        'VSA_EState3','VSA_EState4','VSA_EState5','VSA_EState6','VSA_EState7',\n        'VSA_EState8','VSA_EState9','fr_Ar_N','fr_C_O','fr_NH0','fr_NH1',\n        'fr_aniline','fr_ether','fr_halogen','fr_thiophene'\n    ]).union(required_descriptors)),\n\n    'Tc': list(set([\n        'BalabanJ','BertzCT','Chi0','EState_VSA5','ExactMolWt','FpDensityMorgan1',\n        'FpDensityMorgan2','FpDensityMorgan3','HeavyAtomMolWt','MinEStateIndex',\n        'MolWt','NumAtomStereoCenters','NumRotatableBonds','NumValenceElectrons',\n        'SMR_VSA10','SMR_VSA7','SPS','SlogP_VSA6','SlogP_VSA8','VSA_EState1',\n        'VSA_EState7','fr_NH1','fr_ester','fr_halogen'\n    ]).union(required_descriptors)),\n\n    'Density': list(set([\n        'BalabanJ','Chi3n','Chi3v','Chi4n','EState_VSA1','ExactMolWt',\n        'FractionCSP3','HallKierAlpha','Kappa2','MinEStateIndex','MolMR','MolWt',\n        'NumAliphaticCarbocycles','NumHAcceptors','NumHeteroatoms',\n        'NumRotatableBonds','SMR_VSA10','SMR_VSA5','SlogP_VSA12','SlogP_VSA5',\n        'TPSA','VSA_EState10','VSA_EState7','VSA_EState8'\n    ]).union(required_descriptors)),\n\n    'Rg': list(set([\n        'AvgIpc','Chi0n','Chi1v','Chi2n','Chi3v','ExactMolWt','FpDensityMorgan1',\n        'FpDensityMorgan2','FpDensityMorgan3','HallKierAlpha','HeavyAtomMolWt',\n        'Kappa3','MaxAbsEStateIndex','MolWt','NOCount','NumRotatableBonds',\n        'NumUnspecifiedAtomStereoCenters','NumValenceElectrons','PEOE_VSA14',\n        'PEOE_VSA6','SMR_VSA1','SMR_VSA5','SPS','SlogP_VSA1','SlogP_VSA2',\n        'SlogP_VSA7','SlogP_VSA8','VSA_EState1','VSA_EState8','fr_alkyl_halide',\n        'fr_halogen'\n    ]).union(required_descriptors))\n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:39:08.049849Z","iopub.execute_input":"2025-07-16T12:39:08.050224Z","iopub.status.idle":"2025-07-16T12:39:08.076098Z","shell.execute_reply.started":"2025-07-16T12:39:08.050199Z","shell.execute_reply":"2025-07-16T12:39:08.074562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.mixture import GaussianMixture\n\ndef augment_dataset(X, y, n_samples=1000, n_components=5, random_state=None):\n    \"\"\"\n    Augments a dataset using Gaussian Mixture Models.\n\n    Parameters:\n    - X: pd.DataFrame or np.ndarray ‚Äî feature matrix\n    - y: pd.Series or np.ndarray ‚Äî target values\n    - n_samples: int ‚Äî number of synthetic samples to generate\n    - n_components: int ‚Äî number of GMM components\n    - random_state: int ‚Äî random seed for reproducibility\n\n    Returns:\n    - X_augmented: pd.DataFrame ‚Äî augmented feature matrix\n    - y_augmented: pd.Series ‚Äî augmented target values\n    \"\"\"\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n    elif not isinstance(X, pd.DataFrame):\n        raise ValueError(\"X must be a pandas DataFrame or a NumPy array\")\n\n    X.columns = X.columns.astype(str)\n\n    if isinstance(y, np.ndarray):\n        y = pd.Series(y)\n    elif not isinstance(y, pd.Series):\n        raise ValueError(\"y must be a pandas Series or a NumPy array\")\n\n    df = X.copy()\n    df['Target'] = y.values\n\n    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n    gmm.fit(df)\n\n    synthetic_data, _ = gmm.sample(n_samples)\n    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n\n    augmented_df = pd.concat([df, synthetic_df], ignore_index=True)\n\n    X_augmented = augmented_df.drop(columns='Target')\n    y_augmented = augmented_df['Target']\n\n    return X_augmented, y_augmented\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:39:08.077136Z","iopub.execute_input":"2025-07-16T12:39:08.077618Z","iopub.status.idle":"2025-07-16T12:39:08.10645Z","shell.execute_reply.started":"2025-07-16T12:39:08.077593Z","shell.execute_reply":"2025-07-16T12:39:08.105354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.feature_selection import VarianceThreshold\n\n\ntrain_df=train_extended\ntest_df=test\nsubtables = separate_subtables(train_df)\n\ntest_smiles = test_df['SMILES'].tolist()\ntest_ids = test_df['id'].values\nlabels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n#labels = ['Tc']\n\noutput_df = pd.DataFrame({\n\t'id': test_ids\n})\n\n\nfor label in labels:\n\tprint(f\"Processing label: {label}\")\n\tprint(subtables[label].head())\n\tprint(subtables[label].shape)\n\toriginal_smiles = subtables[label]['SMILES'].tolist()\n\toriginal_labels = subtables[label][label].values\n\n\toriginal_smiles, original_labels = augment_smiles_dataset(original_smiles, original_labels, num_augments=1)\n\tfingerprints, descriptors, valid_smiles, invalid_indices\\\n\t\t=smiles_to_combined_fingerprints_with_descriptors(original_smiles, radius=2, n_bits=128)\n\t# descriptors, valid_smiles, invalid_indices\\\n\t#\t =smiles_to_descriptors_with_fingerprints(original_smiles, radius=2, n_bits=128)\n\n\tX=pd.DataFrame(descriptors)\n\tX=X.drop(['BCUT2D_MWLOW','BCUT2D_MWHI','BCUT2D_CHGHI','BCUT2D_CHGLO','BCUT2D_LOGPHI','BCUT2D_LOGPLOW','BCUT2D_MRLOW','BCUT2D_MRHI','MinAbsPartialCharge','MaxPartialCharge','MinPartialCharge','MaxAbsPartialCharge', 'SMILES'],axis=1)\n\ty = np.delete(original_labels, invalid_indices)\n\t\n\t# pd.DataFrame(X).to_csv(f\"./mats/{label}.csv\")\n\t# pd.DataFrame(y).to_csv(f\"./mats/{label}label.csv\", header=None)\n\t\n\t# binned = pd.qcut(y, q=10, labels=False, duplicates='drop')\n\t# pd.DataFrame(binned).to_csv(f\"./mats/{label}integerlabel.csv\", header=None, index=False)\n\tX = X.filter(filters[label])\n\t# Convert fingerprints array to DataFrame\n\tfp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fingerprints.shape[1])])\n\n\tprint(fp_df.shape)\n\t# Reset index to align with X\n\tfp_df.reset_index(drop=True, inplace=True)\n\tX.reset_index(drop=True, inplace=True)\n\t# Concatenate descriptors and fingerprints\n\tX = pd.concat([X, fp_df], axis=1)\n    \n\tprint(f\"After concat: {X.shape}\")\n\t\n\t# Set the variance threshold\n\tthreshold = 0.01\n\n\t# Apply VarianceThreshold\n\tselector = VarianceThreshold(threshold=threshold)\n\t\n\tX = selector.fit_transform(X)\n\n\tprint(f\"After variance cut: {X.shape}\")\n\n\t# Assuming you have X and y loaded\n    \n\tn_samples = 1000\n\n\tX, y = augment_dataset(X, y, n_samples=n_samples)\n\tprint(f\"After augment cut: {X.shape}\")\n\n\n\tX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=10)\n\t\n\tif label==\"Tg\":\n\t\tModel= XGBRegressor(n_estimators= 2173, learning_rate= 0.0672418745539774, max_depth= 6, reg_lambda= 5.545520219149715)\n\tif label=='Rg':\n\t\tModel = XGBRegressor(n_estimators= 520, learning_rate= 0.07324113948440986, max_depth= 5, reg_lambda=0.9717380315982088)\n\tif label=='FFV':\n# Best parameters found: {'n_estimators': 2202, 'learning_rate': 0.07220580588586338, 'max_depth': 4, 'reg_lambda': 2.8872976032666493}\n\t\tModel = XGBRegressor(n_estimators= 2202, learning_rate= 0.07220580588586338, max_depth= 4, reg_lambda= 2.8872976032666493)\n\tif label=='Tc':\n\t\tModel = XGBRegressor(n_estimators= 1488, learning_rate= 0.010456188013762864, max_depth= 5, reg_lambda= 9.970345982204618)\n#Best parameters found: {'n_estimators': 1488, 'learning_rate': 0.010456188013762864, 'max_depth': 5, 'reg_lambda': 9.970345982204618}\n\tif label=='Density':\n\t\tModel = XGBRegressor(n_estimators= 1958, learning_rate= 0.10955287548172478, max_depth= 5, reg_lambda= 3.074470087965767)\n\n\tModel.fit(X_train,y_train)\n\ty_pred=Model.predict(X_test)\n\tprint(mean_absolute_error(y_pred,y_test))\n\n\tModel.fit(X,y)\n\t# Predict on test set\n\t#test_smiles = test_df['SMILES'].str.replace('*', 'C')\n\n\tfingerprints, descriptors, valid_smiles, invalid_indices\\\n\t\t=smiles_to_combined_fingerprints_with_descriptors(test_smiles, radius=2, n_bits=128)\n\ttest=pd.DataFrame(descriptors)\n\ttest=test.drop(['BCUT2D_MWLOW','BCUT2D_MWHI','BCUT2D_CHGHI','BCUT2D_CHGLO','BCUT2D_LOGPHI','BCUT2D_LOGPLOW','BCUT2D_MRLOW','BCUT2D_MRHI','MinAbsPartialCharge','MaxPartialCharge','MinPartialCharge','MaxAbsPartialCharge', 'SMILES'],axis=1)\n\n\ttest = test.filter(filters[label])\n    # Convert fingerprints array to DataFrame\n\tfp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fingerprints.shape[1])])\n    \n\t# Reset index to align with X\n\tfp_df.reset_index(drop=True, inplace=True)\n\ttest.reset_index(drop=True, inplace=True)\n    # Concatenate descriptors and fingerprints\n\ttest = pd.concat([test, fp_df], axis=1)\n\ttest = selector.transform(test)\n\tprint(test.shape)\n\n\ty_pred=Model.predict(test).flatten()\n\tprint(y_pred)\n\n\n\tnew_column_name = label\n\toutput_df[new_column_name] = y_pred\n\nprint(output_df)\n\n\noutput_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:39:08.108899Z","iopub.execute_input":"2025-07-16T12:39:08.109197Z"}},"outputs":[],"execution_count":null}]}